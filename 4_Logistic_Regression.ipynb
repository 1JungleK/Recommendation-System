{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8291b7",
   "metadata": {},
   "source": [
    "相比协同过滤模型仅利用用户与物品的相互行为信息进行推荐, 逻辑回归模型能够综合利用用户、物品、上下文等多种不同的特征, 生成较为“全面”的推荐结果。\n",
    "\n",
    "相比协同过滤和矩阵分解利用用户和物品的“相似度”进行推荐, 逻辑回归将推荐问题看成一个分类问题,通过预测正样本的概率对物 品进行排序。这里的正样本可以是用户“点击”了某商品,也可以是用户“观看”了某视频,均是推荐系统希望用户产生的“正反馈”行为。因此,逻辑回归模型将推荐问题转换成了一个点击率(Click Through Rate,CTR)预估问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa9b51",
   "metadata": {},
   "source": [
    "基于逻辑回归的推荐过程如下  \n",
    "1. 将用户年龄、性别、物品属性、物品描述、当前时间、当前 地点等特征转换成数值型特征向量。  \n",
    "2. 确定逻辑回归模型的优化目标(以优化“点击率”为例),利用已有样本数据对逻辑回归模型进行训练,确定逻辑回归模型的内部 参数。  \n",
    "3. 在模型服务阶段,将特征向量输入逻辑回归模型,经过逻辑回归模型的推断,得到用户“点击”(这里用点击作为推荐系统正反馈行为的例子)物品的概率。 \n",
    "4. 利用“点击”概率对所有候选物品进行排序,得到推荐列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03633005",
   "metadata": {},
   "source": [
    "逻辑回归模型：\n",
    "\n",
    "$$ y = sigmoid(wx+b) $$\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb05512",
   "metadata": {},
   "source": [
    "Lost function of Logistic Regression:\n",
    "\n",
    "`极大似然估计`：\n",
    "概率（Probability）：描述的是在已知模型参数的情况下，观察到某数据的可能性。例如，已知一个硬币是公平的（即正面朝上的概率θ=0.5），那么抛三次得到两次正面的概率可以用二项分布计算。\n",
    "\n",
    "似然（Likelihood）：与概率相反，似然描述的是在已知观测数据的情况下，模型参数取某个值的可能性。也就是说，似然函数是关于参数的函数，给定数据后，我们可以通过似然函数来评估不同参数值的合理性。\n",
    "\n",
    "极大似然估计（Maximum Likelihood Estimation, MLE）是一种统计方法，用于估计模型的参数。其核心思想是：在所有可能的参数值中，选择使得观测数据出现的概率（即似然函数）最大的那个参数值作为参数的估计。\n",
    "\n",
    "似然函数：\n",
    "$$ L(\\theta; X_1,...,X_n) = \\prod_{i=1}^n P(X_i \\mid \\theta) $$\n",
    "\n",
    "\n",
    "概率 vs. 似然：硬币抛掷例子\n",
    "\n",
    " 问题设定\n",
    "- **实验**：抛掷一枚硬币10次，观察到7次正面（H），3次反面（T）\n",
    "- **模型**：伯努利分布，正面概率为θ，反面概率为1-θ\n",
    "\n",
    "1. 概率（Probability）视角\n",
    "已知条件\n",
    "- 假设硬币是公平的：θ = 0.5（固定参数）\n",
    "\n",
    "计算目标\n",
    "求观察到7次正面的概率：\n",
    "\n",
    "$$\n",
    "P(X=7 \\mid θ=0.5) = \\binom{10}{7}0.5^7(1-0.5)^3 \\approx 0.117\n",
    "$$\n",
    "\n",
    "解释\n",
    "当硬币公平时，出现7正3反的概率约为11.7%。\n",
    "\n",
    "2. 似然（Likelihood）视角\n",
    "已知条件\n",
    "- 观测数据：7次正面，3次反面（固定数据）\n",
    "\n",
    "似然函数\n",
    "$$\n",
    "L(θ \\mid X=7) = θ^7(1-θ)^3\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafeee72",
   "metadata": {},
   "source": [
    "因为似然函数，即目标函数的连乘不利于求导，一般使用对数形式，再乘以$-\\frac{1}{m} 转化为最小话损失函数问题$：\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right] $$\n",
    "\n",
    "其中$h$是sigmoid函数\n",
    "\n",
    "$y^i$是真实标签（0，1）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d837c",
   "metadata": {},
   "source": [
    "使用梯度下降法优化参数（$w$），可考虑加入正则化项以防止过拟合现象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305fe1a",
   "metadata": {},
   "source": [
    "逻辑回归模型的优势：\n",
    "1. 数学含义上的支撑\n",
    "2. 可解释性强\n",
    "3. 工程化的需要\n",
    "\n",
    "劣势：\n",
    "1. 表达能力相较于神经网络来说不足\n",
    "2. 无法进行特征筛选、交叉"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
